{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkQj/Pl/nbAn0+w0aPZd7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d98s/TrialNotebook/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZxAUuBSzQ8h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# data_dir = '../Downloads/annotations_trainval2014'\n",
        "data_dir = './'\n",
        "data_type = 'val2014'\n",
        "anno = '{}/annotations/instances_{}.json'.format(data_dir, data_type)\n",
        "coco = COCO(anno)\n",
        "categories = coco.loadCats(coco.getCatIds())\n",
        "category_names = [cats['name'] for cats in categories]\n",
        "f = open(anno)\n",
        "annotations = json.load(f)"
      ],
      "metadata": {
        "id": "DNfNhZlXh6LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_id = {}\n",
        "cat_box = {}\n",
        "for annot in annotations['annotations']:\n",
        "    if annot['category_id'] not in cat_id.keys():\n",
        "        cat_id[annot['category_id']] = []\n",
        "        cat_box[annot['category_id']] = []\n",
        "    cat_id[annot['category_id']].append(annot['image_id'])\n",
        "    cat_box[annot['category_id']].append(annot['bbox'])\n",
        "\n",
        "indices = torch.randint(2000, (1000,))\n",
        "\n",
        "arranged_id = []\n",
        "arranged_box = []\n",
        "for idx in indices:\n",
        "    arranged_id.append(cat_id[1][idx])\n",
        "    arranged_id.append(cat_id[2][idx])\n",
        "    arranged_id.append(cat_id[3][idx])\n",
        "    arranged_box.append(cat_box[1][idx])\n",
        "    arranged_box.append(cat_box[2][idx])\n",
        "    arranged_box.append(cat_box[3][idx])"
      ],
      "metadata": {
        "id": "HN9VxWDwiQs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CocoDataset():\n",
        "    def __init__(self, img_ids, bboxes, img_path):\n",
        "        self.img_ids  = img_ids\n",
        "        self.bboxes = bboxes\n",
        "        self.img_path = img_path\n",
        "        self.prefix = '000000000000'\n",
        "        self.len = len(self.img_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        img_name = self.prefix[0:12-len(str(img_id))] + str(img_id) + '.jpg'\n",
        "        img = np.asarray(Image.open(self.img_path + 'COCO_val2014_' + img_name))\n",
        "        x, y, w, h = int(self.bboxes[idx][0]), int(self.bboxes[idx][1]), int(self.bboxes[idx][2]), int(self.bboxes[idx][3])\n",
        "        w = w+1 if w==0 else w\n",
        "        h = h+1 if h==0 else h\n",
        "        img = processor(Image.fromarray(img[y:y+h,x:x+w]))\n",
        "        return img"
      ],
      "metadata": {
        "id": "XmtDMRfhlTGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  def __init__(self, clip_model):\n",
        "    super(TextEncoder, self).__init__()\n",
        "    self.positional_embedding = clip_model.positional_embedding\n",
        "    self.transformer = clip_model.transformer\n",
        "    self.ln_final = clip_model.ln_final\n",
        "    self.text_projection = clip_model.text_projection\n",
        "    self.dtype = clip_model.dtype\n",
        "\n",
        "  def forward(self, prompt_embeddings, prompt_tokens):\n",
        "    x = prompt_embeddings + self.posiional_embedding.type(self.dtype)\n",
        "    x = x.permute(1,0,2)    # NLD -> LND\n",
        "    x = self.transformer(x)\n",
        "    x = x.permute(1,0,2)    # LND -> NLD\n",
        "    x = self.ln_final(x).type(self.dtype)\n",
        "    x =x[torch.arange(x.shape[0]), prompt_tokens.argmax(dim=-1)] @ self.text_projection.type(self.dtype)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1RyMUx5jzuKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prompt(nn.Module):\n",
        "  def __init__(self, clip_model, n_ctx, class_names):\n",
        "    super(Prompt, self).__init__()\n",
        "    self.n_ctx = n_ctx\n",
        "    self.class_names= class_names\n",
        "    self.n_class = len(self.class_names)\n",
        "    self.dtype = clip_model.dtype\n",
        "    ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "    ctx_vec = torch.empty(self.n_ctx, ctx_dim, dtype=self.dtype)\n",
        "    nn.init.normal_(ctx_vec, std=0.02)\n",
        "    prompt_prefix = \" \".join([\"X\"] * self.n_ctx)\n",
        "    self.ctx = nn.Parameter(ctx_vec)\n",
        "    prompts = [prompt_prefix + \" \" + names + \".\" for names in self.calss_names]\n",
        "    self.prompt_tokens = torch.cat([clip.tokenize(prompt) for prompt in prompts])\n",
        "    with torch.no_grad():\n",
        "      embedding = clip_model.token_embedding(self.prompt_tokens).type(self.dtype)\n",
        "    self.prefix_embedding = embedding[:, :1, :]\n",
        "    self.suffix_embedding = embedding[:, n_ctx+1:, :]\n",
        "\n",
        "  def forward(self):\n",
        "    ctx = self.ctx.unsqueeze(0).expand(self.n_class, -1, -1)\n",
        "    prompt_embeddings = torch.cat([self.prefix_embedding, self.ctx, self.suffix_embedding], dim=1)\n",
        "    return prompt_embeddings"
      ],
      "metadata": {
        "id": "X3PH3EyAzSd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "  def __init__(self, clip_model, n_ctx, class_names):\n",
        "    self.prompt = Prompt(clip_model, n_ctx, class_names)\n",
        "    self.prompt_tokens = self.prompt.prompt_tokens\n",
        "    self.text_encoder = TextEncoder(clip_model)\n",
        "    self.image_encoder = clip_model.visual\n",
        "    self.dtype = clip_model.dtype\n",
        "\n",
        "  def forward(self, images):\n",
        "    prompt_embeddings = self.prompt()\n",
        "    text_encodings = self.text_encoder(prompt_embeddings, self.prompt_tokens)\n",
        "    text_encodings /= text_encodings.norm(dim=-1, keepdim=True)\n",
        "    image_encodings = self.image_encoder(images.type(self.dtype))\n",
        "    image_encodings /= image_encodings.norm(dim=-1, keepdim=True)\n",
        "    sim = image_encodings @ text_encodings.T\n",
        "    return sim"
      ],
      "metadata": {
        "id": "1evXcZaUaWAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(custom_clip, dataloader, optim, criterion, num_epochs, device):\n",
        "  clip_model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    print('epoch ', epoch)\n",
        "    running_loss = 0\n",
        "    for inputs in tqdm(dataloader):\n",
        "      images = inputs.to(device)\n",
        "      optim.zero_grad()\n",
        "      sim = custom_clip(images)\n",
        "      pred = sim.argmax(dim=-1)\n",
        "      loss = criterion(pred, torch.arange(80))\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      print('loss: ', loss.item())\n",
        "      # running_loss += loss.item()*inputs.size(0)"
      ],
      "metadata": {
        "id": "5gBc1s8rkXQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_parameters(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "j0ZPam7mi_Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "clip_model, processor = clip.load('ViT-B/32', device=device)"
      ],
      "metadata": {
        "id": "iKhz1aZUjH9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wbS87TdjI4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CocoDataset(arranged_id, arranged_box, './val2014/')\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, shuffle=False)"
      ],
      "metadata": {
        "id": "SbxypyYrirVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_ctx = 5\n",
        "custom_clip = CustomCLIP(clip_model, n_ctx, category_names).to(device)\n",
        "optimizer = torch.optim.Adam(custom_clip.prompt.ctx, lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "train(clip_model, dataloader, optimizer, criterion, 200, device)"
      ],
      "metadata": {
        "id": "EmXSqgRKjJ9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}